# Compatibility patch for litellm/openai incompatibility
# This must be done before any imports that might load litellm
def patch_openai_responses():
    """Patch OpenAI module to fix ResponseTextConfig import issue with litellm."""
    try:
        # Import and patch the response module
        import openai.types.responses.response as response_module
        
        # Check if ResponseTextConfig is missing and ResponseFormatTextConfig exists
        if not hasattr(response_module, 'ResponseTextConfig') and hasattr(response_module, 'ResponseFormatTextConfig'):
            # Alias ResponseFormatTextConfig as ResponseTextConfig
            response_module.ResponseTextConfig = response_module.ResponseFormatTextConfig
            
        # Import and patch the response_create_params module
        import openai.types.responses.response_create_params as params_module
        
        # Check if ResponseTextConfigParam is missing and ResponseFormatTextConfigParam exists
        if not hasattr(params_module, 'ResponseTextConfigParam') and hasattr(params_module, 'ResponseFormatTextConfigParam'):
            # Alias ResponseFormatTextConfigParam as ResponseTextConfigParam
            params_module.ResponseTextConfigParam = params_module.ResponseFormatTextConfigParam
            
        print("Applied compatibility patch for OpenAI/LiteLLM")
    except Exception as e:
        print(f"Warning: Could not apply OpenAI patch: {e}")

# Apply the patch before any other imports
patch_openai_responses()

# Now continue with normal imports
import json
import requests
import pandas as pd
import spacy
from scispacy.linking import EntityLinker
import dspy
from typing import List, Dict, Any, Optional, Generator, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
# from itertools import combinations  # Not needed for current relationship implementation
import os
import threading
from relationship_generator import MedicalRelationshipGenerator, create_relationship_generator
import logging
import numpy as np
import ast
import re
import datetime
from collections import defaultdict
from embedding_pipeline import scibertEmbedder
from summarize_entities import EntitySummarizer
import warnings
from sklearn.exceptions import InconsistentVersionWarning
# from tqdm import tqdm  # Not needed for current relationship implementation
# import random  # Not needed for current relationship implementation

# Suppress the specific TfidfVectorizer version warning from scispacy models
warnings.filterwarnings('ignore', category=InconsistentVersionWarning, 
                       message='.*TfidfVectorizer.*version.*1.1.2.*1.7.1.*')

# Optimize spacy for speed
spacy.prefer_gpu()  # Use GPU if available

# Configure enhanced logging with file output
def setup_logging():
    """Setup enhanced logging with both console and file output."""
    os.makedirs('./logs', exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"./logs/entity_processing_improved_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"Logging initialized. Log file: {log_filename}")
    return logger

logger = setup_logging()

class CUISelectionSignature(dspy.Signature):
    """DSPy signature for selecting the best CUI from candidates based on entity description."""
    
    entity_name = dspy.InputField(desc="The medical entity name/text")
    entity_description = dspy.InputField(desc="Detailed description of the entity generated by LLM")
    cui_candidates = dspy.InputField(desc="""JSON list of top CUI candidates with their details. Format:
    [
        {"cui": "C0038894", "score": 0.987, "semantic_type": "Biomedical Occupation or Discipline", "definition": "The branch of medical science that treats disease or injury by operative procedures."},
        {"cui": "C0543467", "score": 0.987, "semantic_type": "Therapeutic or Preventive Procedure", "definition": "A medical procedure involving operative techniques."}
    ]
    """)
    
    selected_cui = dspy.OutputField(desc="""Return ONLY a JSON object with the selected CUI:
    {
        "selected_cui": "C0543467",
        "reasoning": "Brief explanation of why this CUI was selected based on the entity description"
    }
    """)



class MedicalEntityExtractor:
    """
    Dedicated component for medical entity extraction with enhanced CUI assignment.
    
    Specialized for interpreting medical texts including:
    - Clinical discharge notes
    - Medical textbooks
    - Clinical laboratory reports
    - Clinical guideline interpretation
    
    Processing flow:
    1. Detect entities with SciSpacy NER
    2. Generate medical descriptions using LLM
    3. Re-assign CUIs using descriptions as context for accurate mapping
    """
    
    def __init__(self, processor_instance):
        """Initialize with reference to main processor instance."""
        self.processor = processor_instance
    
    def extract_medical_entities(self, paragraph_content: str) -> Dict[str, Any]:
        """
        Extract medical entities with improved CUI assignment using description context.
        
        Specialized for medical text interpretation:
        - Clinical discharge notes: Focus on diagnoses, medications, procedures
        - Medical textbooks: Emphasize anatomical structures, diseases, treatments
        - Clinical labs: Prioritize test results, measurements, reference values
        - Clinical guidelines: Target recommendations, contraindications, protocols
        
        Args:
            paragraph_content: Medical text content to analyze
            
        Returns:
            Dict containing extracted entities with enhanced medical context
        """
        try:
            # FIRST PASS: Extract entities and get ALL CUI candidates
            doc = self.processor.nlp(paragraph_content)
            
            detected_entities = []
            for ent in doc.ents:
                if len(ent.text.strip()) < 2:
                    continue
                
                # Store ALL CUI candidates for medical context analysis
                cui_candidates = []
                if self.processor.linker and hasattr(ent._, 'kb_ents') and len(ent._.kb_ents) > 0:
                    cui_candidates = list(ent._.kb_ents)
                
                # FILTER: Only include entities that have CUI candidates
                # Essential for medical text where UMLS mapping is critical
                if not cui_candidates:
                    logger.debug(f"Skipping medical entity '{ent.text.strip()}' - no UMLS CUI candidates found")
                    continue
                
                detected_entities.append({
                    'entity_name': ent.text.strip(),
                    'cui_candidates': cui_candidates,
                    'spacy_label': ent.label_,
                    'start_char': ent.start_char,
                    'end_char': ent.end_char
                })
            
            if not detected_entities:
                return {'entities': []}
            
            logger.info(f"Medical entity detection: Found {len(detected_entities)} entities with UMLS candidates")
            
            # SECOND PASS: Generate medical descriptions using specialized LLM prompts
            entity_descriptions = {}
            if self.processor.ollama_model:
                batch_results = self._generate_medical_entity_descriptions(paragraph_content, detected_entities)
                entity_descriptions = batch_results.get("entity_descriptions", {})
            
            # THIRD PASS: Re-assign CUIs using medical context descriptions
            final_entities = []
            
            for entity in detected_entities:
                entity_name = entity['entity_name']
                entity_description = entity_descriptions.get(entity_name, "")
                cui_candidates = entity['cui_candidates']
                
                if self.processor.use_description_context and entity_description and cui_candidates:
                    # Use LLM-based selection optimized for medical entities
                    cui, score, entity_type, umls_definition = self.processor.select_best_cui_with_llm(
                        entity_name, 
                        entity_description,
                        cui_candidates,
                        paragraph_content
                    )
                    
                    # Log medical entity selection details
                    if len(cui_candidates) > 1:
                        original_cui = cui_candidates[0][0] if cui_candidates else ""
                        if cui != original_cui:
                            logger.info(f"Medical context CUI selection for '{entity_name}': {original_cui} -> {cui}")
                elif cui_candidates:
                    # Fallback to first candidate with medical semantic type lookup
                    cui, score = cui_candidates[0]
                    entity_type, umls_definition = self.processor._get_semantic_info_with_fallback(cui)
                else:
                    cui, score, entity_type, umls_definition = "", 0.0, "Entity", ""
                
                # Generate medical-focused embedding if available
                embedding = []
                if self.processor.embedder and entity_description:
                    try:
                        entity_embedding = self.processor.embedder.encode_entity(entity_name, entity_type, entity_description)
                        embedding = entity_embedding.tolist() if entity_embedding is not None else []
                    except Exception as e:
                        logger.warning(f"Failed to compute medical embedding for '{entity_name}': {e}")
                
                # Extract content sentence for relationship analysis
                content_sentence = ""
                try:
                    context_dict = self._extract_entity_context_sentences(
                        paragraph_content, 
                        entity['start_char'], 
                        entity['end_char']
                    )
                    content_sentence = context_dict.get('current_sentence', '')
                except Exception as e:
                    logger.warning(f"Failed to extract content sentence for '{entity_name}': {e}")
                
                final_entities.append({
                    'entity_name': entity_name,
                    'entity_description': entity_description,
                    'entity_type': entity_type,
                    'umls_definition': umls_definition,
                    'cui': cui,
                    'linker_score': score,
                    'spacy_label': entity['spacy_label'],
                    'start_char': entity['start_char'],
                    'end_char': entity['end_char'],
                    'content_sentence': content_sentence,
                    'is_clinically_relevant': True,
                    'content_embedding': embedding,
                    'cui_assignment_method': 'medical-llm-enhanced' if self.processor.use_description_context else 'standard',
                    'total_cui_candidates': len(entity['cui_candidates'])
                })
            
            return {'entities': final_entities}
            
        except Exception as e:
            logger.error(f"Failed to extract medical entities with improved CUI: {e}")
            return {'entities': []}
    
    def _extract_entity_context_sentences(self, text: str, start_char: int, end_char: int) -> Dict[str, str]:
        """
        Extract the sentence containing the entity plus the sentences before and after it.
        Returns separated current sentence and surrounding context.
        
        Args:
            text: Full paragraph text
            start_char: Starting character position of entity
            end_char: Ending character position of entity
            
        Returns:
            Dict with 'current_sentence', 'surrounding_context', and 'full_context'
        """
        try:
            import re
            
            # Split text into sentences while preserving punctuation
            # Use positive lookahead to keep the punctuation with the sentence
            sentences = re.split(r'(?<=[.!?])\s+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            # Find which sentence contains the entity
            entity_sentence_idx = -1
            char_count = 0
            
            for i, sentence in enumerate(sentences):
                sentence_start = char_count
                sentence_end = char_count + len(sentence)
                
                # Check if entity falls within this sentence
                if (start_char >= sentence_start and start_char <= sentence_end) or \
                   (end_char >= sentence_start and end_char <= sentence_end) or \
                   (start_char <= sentence_start and end_char >= sentence_end):
                    entity_sentence_idx = i
                    break
                
                # Add length of sentence plus separator (period/space)
                char_count += len(sentence) + 2  # Approximate for punctuation and space
            
            if entity_sentence_idx == -1:
                # Fallback: return first 300 characters if sentence detection fails
                fallback_text = text[:300] + "..." if len(text) > 300 else text
                return {
                    'current_sentence': fallback_text,
                    'surrounding_context': '',
                    'full_context': fallback_text
                }
            
            # Extract separated contexts
            current_sentence = sentences[entity_sentence_idx]
            surrounding_sentences = []
            
            # Previous sentence (if exists)
            if entity_sentence_idx > 0:
                surrounding_sentences.append(sentences[entity_sentence_idx - 1])
            
            # Next sentence (if exists)  
            if entity_sentence_idx < len(sentences) - 1:
                surrounding_sentences.append(sentences[entity_sentence_idx + 1])
            
            surrounding_context = ' '.join(surrounding_sentences) if surrounding_sentences else ''
            
            # Also create full context for backward compatibility
            context_sentences = []
            if entity_sentence_idx > 0:
                context_sentences.append(sentences[entity_sentence_idx - 1])
            context_sentences.append(sentences[entity_sentence_idx])
            if entity_sentence_idx < len(sentences) - 1:
                context_sentences.append(sentences[entity_sentence_idx + 1])
            full_context = ' '.join(context_sentences)
            
            return {
                'current_sentence': current_sentence,
                'surrounding_context': surrounding_context,
                'full_context': full_context
            }
            
        except Exception as e:
            logger.warning(f"Failed to extract entity context sentences: {e}")
            # Fallback to first 300 characters
            fallback_text = text[:300] + "..." if len(text) > 300 else text
            return {
                'current_sentence': fallback_text,
                'surrounding_context': '',
                'full_context': fallback_text
            }

    def _generate_medical_entity_descriptions(self, paragraph_content: str, detected_entities: List[Dict]) -> Dict[str, Any]:
        """
        Generate medical descriptions for entities using specialized LLM prompts.
        
        Optimized for medical text types:
        - Clinical discharge notes: Focus on patient context and clinical significance
        - Medical textbooks: Emphasize educational definitions and classifications
        - Clinical labs: Highlight normal/abnormal values and clinical interpretation
        - Clinical guidelines: Target therapeutic recommendations and contraindications
        """
        try:
            entity_descriptions = {}
            
            # Check cache first
            for entity in detected_entities:
                entity_name = entity['entity_name']
                if entity_name in self.processor.desc_cache:
                    entity_descriptions[entity_name] = self.processor.desc_cache[entity_name]
            
            # Generate descriptions for uncached entities
            uncached_entities = [e for e in detected_entities if e['entity_name'] not in entity_descriptions]
            
            if uncached_entities and self.processor.ollama_model:
                # Process in chunks with medical-specific prompting
                for chunk_start in range(0, len(uncached_entities), self.processor.chunk_size):
                    chunk_end = min(chunk_start + self.processor.chunk_size, len(uncached_entities))
                    entity_chunk = uncached_entities[chunk_start:chunk_end]
                    
                    # Call medical-specialized LLM description generator with entity positions
                    entity_input_list = [
                        {
                            "entity_name": ent["entity_name"],
                            "spacy_label": ent["spacy_label"],
                            "start_char": ent["start_char"],
                            "end_char": ent["end_char"]
                        } for ent in entity_chunk
                    ]
                    
                    processed_data = self._ollama_generate_medical_descriptions(
                        paragraph_content, entity_input_list
                    )
                    
                    # Update descriptions and cache
                    for entity_info in processed_data.get("entities", []):
                        entity_name = entity_info["entity_name"]
                        description = entity_info.get("entity_description", "")
                        entity_descriptions[entity_name] = description
                        
                        # Update cache
                        with self.processor.cache_lock:
                            self.processor.desc_cache[entity_name] = description
            
            # Save cache
            self.processor._save_description_cache()
            
            return {"entity_descriptions": entity_descriptions}
            
        except Exception as e:
            logger.error(f"Failed to generate medical entity descriptions: {e}")
            return {"entity_descriptions": {}}
    
    def _ollama_generate_medical_descriptions(self, context_text: str, entity_input_list: list[dict], 
                                     max_retries: int = 3) -> dict:
        """
        Call LLM to generate medical descriptions for entities using specialized medical prompts.
        
        Specialized prompting for different medical text types:
        - Clinical discharge notes: "Describe this entity's clinical significance and role in patient care"
        - Medical textbooks: "Provide comprehensive medical definition and classification"
        - Clinical labs: "Explain clinical interpretation and reference ranges"
        - Clinical guidelines: "Describe therapeutic role and clinical recommendations"
        """
        if not self.processor.ollama_model:
            return {"entities": []}
        
        for attempt in range(max_retries):
            try:
                # Build contextual prompts for each entity using focused sentence context
                prompts_for_entities = []
                
                for entity_info in entity_input_list:
                    entity_name = entity_info["entity_name"]
                    start_char = entity_info.get("start_char", 0)
                    end_char = entity_info.get("end_char", 0)
                    
                    # Extract focused context around this specific entity (now returns dict)
                    entity_context_dict = self._extract_entity_context_sentences(context_text, start_char, end_char)
                    
                    # Medical text type detection for this entity's context
                    text_type = self._detect_medical_text_type(entity_context_dict['full_context'])
                    
                    prompts_for_entities.append({
                        "entity_name": entity_name,
                        "context": entity_context_dict['full_context'],  # For backward compatibility
                        "current_sentence": entity_context_dict['current_sentence'],
                        "surrounding_context": entity_context_dict['surrounding_context'],
                        "text_type": text_type
                    })
                
                # Build combined prompt with contextual information for all entities
                prompt = self._build_contextual_medical_description_prompt(prompts_for_entities)
                
                # Call LLM with medical-optimized parameters
                if self.processor.use_lmstudio:
                    url = f"{self.processor.ollama_endpoint}/v1/chat/completions"
                    payload = {
                        "model": self.processor.ollama_model or "current",
                        "messages": [
                            {"role": "system", "content": "You are an expert specializing in medical text document interpretation. You MUST respond with valid JSON only."},
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.1,  # Lower temperature for medical accuracy
                        "max_tokens": 8000,
                        "stream": False
                    }
                    resp = requests.post(url, json=payload, timeout=300)
                    resp.raise_for_status()
                    data = resp.json()
                    raw_text = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                else:
                    url = self.processor.ollama_endpoint.rstrip('/') + "/api/generate"
                    payload = {
                        "model": self.processor.ollama_model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.1, "num_predict": 8000}  # Conservative for medical accuracy
                    }
                    resp = requests.post(url, json=payload, timeout=300)
                    resp.raise_for_status()
                    data = resp.json()
                    raw_text = data.get("response", "")
                
                result = self.processor._parse_llm_response(raw_text)
                if result.get("entities"):
                    return result
                    
            except Exception as e:
                logger.error(f"Medical LLM description generation attempt {attempt + 1} failed: {e}")
        
        return {"entities": []}
    
    def _detect_medical_text_type(self, text: str) -> str:
        """
        Detect the type of medical text to optimize description generation.
        
        Returns one of:
        - 'discharge_note': Clinical discharge summaries
        - 'textbook': Medical educational content
        - 'lab_report': Laboratory results and interpretations
        - 'clinical_guideline': Treatment guidelines and protocols
        - 'general_medical': Default medical text
        """
        text_lower = text.lower()
        
        # Clinical discharge note indicators
        if any(indicator in text_lower for indicator in 
               ['discharge', 'admitted', 'patient', 'hospital course', 'medications on discharge']):
            return 'discharge_note'
        
        # Medical textbook indicators
        elif any(indicator in text_lower for indicator in 
                ['chapter', 'definition', 'classification', 'etiology', 'pathophysiology']):
            return 'textbook'
        
        # Laboratory report indicators
        elif any(indicator in text_lower for indicator in 
                ['lab', 'test result', 'normal range', 'reference', 'abnormal', 'elevated']):
            return 'lab_report'
        
        # Clinical guideline indicators
        elif any(indicator in text_lower for indicator in 
                ['recommend', 'guideline', 'contraindication', 'protocol', 'treatment', 'therapy']):
            return 'clinical_guideline'
        
        return 'general_medical'

    def _build_contextual_medical_description_prompt(self, prompts_for_entities: List[Dict]) -> str:
        """
        Build specialized prompt using focused contextual sentences for each entity.
        
        Args:
            prompts_for_entities: List of dicts with entity_name, context, and text_type
        """
        base_prompt = "Given the following medical entities and their contextual sentences, provide detailed medical descriptions for each entity.\n\n"
        
        # Add entity-specific contexts with priority indicators
        for i, entity_info in enumerate(prompts_for_entities, 1):
            entity_name = entity_info["entity_name"]
            context = entity_info["context"]
            text_type = entity_info.get("text_type", "general_medical")
            current_sentence = entity_info.get("current_sentence", "")
            surrounding_context = entity_info.get("surrounding_context", "")
            
            base_prompt += f"Entity {i}: '{entity_name}'\n"
            if current_sentence:
                base_prompt += f"PRIMARY CONTEXT (80% focus): {current_sentence}\n"
            if surrounding_context:
                base_prompt += f"SUPPORTING CONTEXT (20% focus): {surrounding_context}\n"
            else:
                base_prompt += f"Context sentences: {context}\n"
            base_prompt += f"Text type: {text_type}\n\n"
        
        # Add instructions with priority emphasis
        base_prompt += """For each entity, provide EXACTLY 2 sentences:
1. First sentence: A comprehensive medical description that MUST explicitly mention the entity name and define what it is, based PRIMARILY on how it appears in the PRIMARY CONTEXT sentence
2. Second sentence: Must start with "In this context," and explain its specific relevance, incorporating supporting details from the context.

CRITICAL REQUIREMENTS:
- Give 80% priority to the PRIMARY CONTEXT sentence where the entity directly appears
- Use SUPPORTING CONTEXT (20%) only to provide additional background
- The description MUST explicitly define and mention the exact entity name provided
- Do not describe other terms from the context sentences
- Focus only on the specific entity requested
- Use the context sentences to understand the entity's role and clinical significance

Return ONLY a JSON object:
{
  "entities": [
    {"entity_name": "entity1", "entity_description": "2 sentences here that explicitly mention and define 'entity1'"},
    {"entity_name": "entity2", "entity_description": "2 sentences here that explicitly mention and define 'entity2'"}
  ]
}"""
        
        return base_prompt




class ImprovedEntityProcessor:
    """
    Enhanced entity processor for medical entity extraction and relationship identification.
    
    Medical Text Interpretation Guidelines:
    - Clinical discharge notes: Focus on patient care sequences, medications, diagnoses
    - Medical textbooks: Emphasize educational content, classifications, definitions
    - Clinical laboratory reports: Highlight test results, reference ranges, abnormalities
    - Clinical guidelines: Target treatment protocols, recommendations, contraindications
    
    Processing flow:
    1. Medical entity extraction with enhanced CUI assignment
    2. Entity relationship generation using LLM analysis (DSPy)
    3. Ensures each entity appears at least once in relationships
    4. Maintains JSON output structure with relationships_within_chunk populated
    """
    
    # Class-level cache for SciSpacy models
    _nlp_cache = {}
    _linker_cache = {}
    
    def __init__(self, spacy_model: str = "en_core_sci_md", ollama_model: str = None, 
                 ollama_endpoint: str = "http://localhost:11434",
                 chunk_size: int = 25,
                 max_workers: Optional[int] = None,
                 progress_interval: int = 20,
                 cache_path: str = "./cache/entity_desc_cache.json",
                 use_description_context: bool = True):
        """
        Initialize the improved entity processor.
        
        Args:
            use_description_context: If True, use LLM descriptions as context for CUI assignment
        """
        self.spacy_model = spacy_model
        self.ollama_model = ollama_model
        self.ollama_endpoint = ollama_endpoint
        self.chunk_size = chunk_size
        self.max_workers = max_workers if max_workers is not None else os.cpu_count() or 4
        self.progress_interval = progress_interval
        self.cache_path = cache_path
        self.use_description_context = use_description_context  # New flag
        # self.relationship_batch_size = relationship_batch_size  # Not needed for current implementation

        # Detect if this is a LM Studio endpoint
        self.use_lmstudio = False
        if ollama_endpoint and '127.0.0.1:1234' in ollama_endpoint:
            self.use_lmstudio = True
            logger.info(f"Detected LM Studio endpoint: {ollama_endpoint}")
        
        # Configure DSPy with the appropriate LLM
        if ollama_model:
            try:
                # Configure DSPy for Ollama
                lm = None
                
                # For Ollama, we need to specify the provider
                if self.use_lmstudio:
                    # LM Studio configuration
                    try:
                        import dspy
                        # For LM Studio, use OpenAI-compatible endpoint
                        lm = dspy.LM(
                            model=f"openai/{ollama_model}",
                            base_url=ollama_endpoint + "/v1",
                            api_key="dummy"  # LM Studio doesn't need a real key
                        )
                        logger.info(f"Using DSPy with LM Studio at {ollama_endpoint}")
                    except Exception as e:
                        logger.warning(f"Failed to configure DSPy for LM Studio: {e}")
                else:
                    # Ollama configuration
                    try:
                        import dspy
                        # For Ollama, specify ollama as provider
                        lm = dspy.LM(
                            model=f"ollama/{ollama_model}",
                            base_url=ollama_endpoint,
                            api_key="dummy"  # Ollama doesn't need a key
                        )
                        logger.info(f"Using DSPy with Ollama model {ollama_model}")
                    except Exception as e:
                        logger.warning(f"Failed to configure DSPy for Ollama: {e}")
                
                if lm:
                    dspy.configure(lm=lm)
                    logger.info(f"[OK] DSPy configured with {ollama_model}")
                else:
                    logger.warning("Could not configure DSPy LM - relationships will use fallback")
                    
            except Exception as e:
                logger.warning(f"Could not configure DSPy: {e}")
                logger.info("DSPy configuration failed - relationships will use direct API fallback")

        self.nlp = None
        self.linker = None
        self.lm = None
        self.batch_predictor = None
        self.semantic_types = []
        self.semantic_type_definitions = {}
        self.semantic_type_categories = {}
        self.semantic_type_cuis = {}
        self.semantic_type_tuis = {}
        self.cui_to_semantic_name = {}
        
        self.embedder = None
        self.summarizer = None
        
        # Global description cache and lock
        self.desc_cache: Dict[str, str] = {}
        self.cache_lock = threading.Lock()
        self._load_description_cache()
        
        # Initialize SciSpacy with entity linker
        try:
            if spacy_model not in self._nlp_cache:
                logger.info(f"Loading SciSpacy model: {spacy_model}")
                self._nlp_cache[spacy_model] = spacy.load(spacy_model)
                
                # Add entity linker
                try:
                    if "scispacy_linker" not in self._nlp_cache[spacy_model].pipe_names:
                        logger.info("Adding SciSpacy entity linker...")
                        self._nlp_cache[spacy_model].add_pipe("scispacy_linker", 
                                                             config={"resolve_abbreviations": True, "linker_name": "umls"})
                        logger.info("[OK] Entity linker added successfully")
                except Exception as linker_error:
                    logger.warning(f"Could not add entity linker: {linker_error}")
                
                logger.info(f"[OK] SciSpacy model cached: {spacy_model}")
            else:
                logger.info(f"[OK] Using cached SciSpacy model: {spacy_model}")
            
            self.nlp = self._nlp_cache[spacy_model]
            
            # Get the linker if it exists
            try:
                if "scispacy_linker" in self.nlp.pipe_names:
                    self.linker = self.nlp.get_pipe("scispacy_linker")
                    logger.info(f"[OK] Entity linker available")
                else:
                    self.linker = None
                    logger.info(f"[OK] No entity linker available")
            except Exception:
                self.linker = None
                
        except Exception as e:
            logger.error(f"Failed to initialize SciSpacy: {e}")
            raise RuntimeError(f"Could not initialize SciSpacy with model {spacy_model}")
        
        # Initialize other components (embedder, summarizer, etc.)
        self._initialize_components()
        
        # Initialize separated medical components
        self.entity_extractor = MedicalEntityExtractor(self)
        self.relationship_generator = create_relationship_generator(
            llm_model=self.ollama_model,
            llm_endpoint=self.ollama_endpoint
        )
    
    def _initialize_components(self):
        """Initialize embedder, summarizer, and other components."""
        # Initialize scibert embedder
        try:
            # Clear any cached models that might be corrupted
            import os
            cache_dir = os.path.expanduser("~/.cache/huggingface/transformers")
            
            # Try to initialize embedder
            self.embedder = scibertEmbedder(batch_size=16, device="cpu")
            logger.info(f"[OK] scibert embedder initialized on CPU")
        except Exception as e:
            logger.error(f"Failed to initialize scibert embedder: {e}")
            logger.error(f"Full error: {type(e).__name__}: {str(e)}")
            
            # Try a manual initialization approach
            try:
                import torch
                from transformers import AutoModel, AutoTokenizer
                logger.info("Trying manual model initialization...")
                
                model_name = "allenai/scibert_scivocab_uncased"
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModel.from_pretrained(model_name)
                
                # Create a simple embedder class
                class SimpleBioMedEmbedder:
                    def __init__(self, tokenizer, model):
                        self.tokenizer = tokenizer
                        self.model = model
                        self.device = "cpu"
                        
                    def encode_entity(self, name, entity_type, description):
                        text = f"{name}. {description}"
                        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
                        with torch.no_grad():
                            outputs = self.model(**inputs)
                        # Mean pooling
                        embeddings = outputs.last_hidden_state.mean(dim=1)
                        return embeddings[0].numpy()
                        
                    def encode(self, texts):
                        results = []
                        for text in texts:
                            inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
                            with torch.no_grad():
                                outputs = self.model(**inputs)
                            embeddings = outputs.last_hidden_state.mean(dim=1)
                            results.append(embeddings[0].numpy())
                        return results
                
                self.embedder = SimpleBioMedEmbedder(tokenizer, model)
                logger.info("[OK] Manual scibert embedder initialized")
                
            except Exception as manual_e:
                logger.error(f"Manual embedder initialization also failed: {manual_e}")
                self.embedder = None

        # Initialize EntitySummarizer
        try:
            if self.use_lmstudio:
                logger.info(f"Initializing EntitySummarizer with LM Studio: model={self.ollama_model}, endpoint={self.ollama_endpoint}")
                self.summarizer = EntitySummarizer(
                    provider="lmstudio",
                    model_id=self.ollama_model or "current",
                    endpoint=self.ollama_endpoint
                )
            else:
                logger.info(f"Initializing EntitySummarizer with Ollama: model={self.ollama_model}, endpoint={self.ollama_endpoint}")
                self.summarizer = EntitySummarizer(
                    ollama_model=self.ollama_model if self.ollama_model else "llama3.1:8b",
                    ollama_endpoint=self.ollama_endpoint
                )
            logger.info("[OK] EntitySummarizer initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize EntitySummarizer: {e}", exc_info=True)
            self.summarizer = None
    
    def select_best_cui_with_llm(self, entity_name: str, entity_description: str, 
                                 cui_candidates: List[Tuple[str, float]],
                                 original_text: str = None) -> Tuple[str, float, str, str]:
        """
        Select the best CUI from candidates using LLM when scores are similar.
        
        Args:
            entity_name: The entity name/text
            entity_description: The LLM-generated description
            cui_candidates: List of (cui, score) tuples from SciSpacy
            original_text: Optional original paragraph text
            
        Returns:
            Tuple of (cui, confidence_score, entity_type, umls_definition)
        """
        if not cui_candidates:
            return "", 0.0, "Entity", ""
        
        # Check if we need LLM selection (top candidates within 0.1 score difference)
        top_candidates = cui_candidates[:5]  # Consider top 5
        if len(top_candidates) > 1:
            score_range = top_candidates[0][1] - top_candidates[-1][1]
            
            if score_range < 0.1:  # Similar scores, use LLM to select
                logger.info(f"Using LLM to select best CUI for '{entity_name}' from {len(top_candidates)} similar candidates")
                
                # Prepare candidate information
                candidate_info = []
                for cui, score in top_candidates:
                    # Use improved semantic info lookup with SciSpacy priority
                    semantic_type, definition = self._get_semantic_info_with_fallback(cui)
                    
                    candidate_info.append({
                        "cui": cui,
                        "score": float(score),
                        "semantic_type": semantic_type,
                        "definition": definition[:200] if definition else ""  # Truncate long definitions
                    })
                
                # Call LLM to select best CUI
                selected_cui = self._llm_select_cui(entity_name, entity_description, candidate_info)
                
                if selected_cui:
                    # Find the selected CUI in candidates
                    for cui, score in top_candidates:
                        if cui == selected_cui:
                            # Get full semantic info with SciSpacy fallback
                            semantic_type, definition = self._get_semantic_info_with_fallback(cui)
                            
                            logger.info(f"LLM selected CUI {cui} ({semantic_type}) for '{entity_name}'")
                            return cui, score, semantic_type, definition
        
        # Default: use top scoring candidate
        cui, score = cui_candidates[0]
        # Use fallback method for semantic info
        semantic_type, definition = self._get_semantic_info_with_fallback(cui)
        
        return cui, score, semantic_type, definition
    
    def _llm_select_cui(self, entity_name: str, entity_description: str, 
                       candidate_info: List[Dict]) -> Optional[str]:
        """
        Use LLM to select the best CUI from candidates based on description.
        """
        if not self.ollama_model:
            return None
        
        try:
            # Format candidates as JSON string
            candidates_json = json.dumps(candidate_info, indent=2)
            
            prompt = f"""You are a medical expert tasked with selecting the most appropriate UMLS CUI (Concept Unique Identifier) for a medical entity.

Entity Name: {entity_name}

Entity Description: {entity_description}

CUI Candidates:
{candidates_json}

Based on the entity description, select the CUI that best matches what the entity represents in this context.

IMPORTANT: Consider whether the description indicates:
- A medical procedure/operation (select Therapeutic or Preventive Procedure)
- A medical specialty/department (select Biomedical Occupation or Discipline)
- An organization/facility (select Health Care Related Organization)
- A clinical finding/symptom (select Finding or Sign or Symptom)

Return ONLY a JSON object with your selection:
{{
    "selected_cui": "C0XXXXXX",
    "reasoning": "Brief explanation of why this CUI best matches the entity description"
}}"""
            
            # Call LLM
            if self.use_lmstudio:
                url = f"{self.ollama_endpoint}/v1/chat/completions"
                payload = {
                    "model": self.ollama_model or "current",
                    "messages": [
                        {"role": "system", "content": "You are a medical informatics expert. Always respond with valid JSON only."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 500,
                    "stream": False
                }
                resp = requests.post(url, json=payload, timeout=30)
                resp.raise_for_status()
                raw_text = resp.json().get("choices", [{}])[0].get("message", {}).get("content", "")
            else:
                url = self.ollama_endpoint.rstrip('/') + "/api/generate"
                payload = {
                    "model": self.ollama_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.1, "num_predict": 500}
                }
                resp = requests.post(url, json=payload, timeout=30)
                resp.raise_for_status()
                raw_text = resp.json().get("response", "")
            
            # Parse response for CUI selection (different format than entity descriptions)
            result = self._parse_cui_selection_response(raw_text)
            selected_cui = result.get("selected_cui", "")
            reasoning = result.get("reasoning", "")
            
            if selected_cui:
                logger.debug(f"LLM selected CUI {selected_cui}: {reasoning}")
                return selected_cui
            else:
                logger.warning(f"LLM did not return a valid CUI selection")
                return None
                
        except Exception as e:
            logger.error(f"Failed to get LLM CUI selection: {e}")
            return None
    
    def _get_semantic_info_with_fallback(self, cui: str) -> Tuple[str, str]:
        """Get semantic type and definition for a CUI with SciSpacy-first approach."""
        
        scispacy_semantic_type = None
        scispacy_definition = None
        
        # First try: Use SciSpacy's UMLS knowledge base (most comprehensive)
        if self.linker and hasattr(self.linker, 'kb') and hasattr(self.linker.kb, 'cui_to_entity'):
            if cui in self.linker.kb.cui_to_entity:
                entity_info = self.linker.kb.cui_to_entity[cui]
                
                # Get definition from SciSpacy
                scispacy_definition = getattr(entity_info, 'definition', '')
                scispacy_canonical_name = getattr(entity_info, 'canonical_name', '')
                
                # PRIORITY: Use SciSpacy TUI-based semantic type (most accurate)
                if hasattr(entity_info, 'types') and entity_info.types:
                    # Use first semantic type with comprehensive TUI mapping
                    tui = entity_info.types[0] if entity_info.types else ''
                    scispacy_semantic_type = self._map_tui_to_semantic_type(tui, scispacy_canonical_name)
                    logger.debug(f"SciSpacy TUI {tui} mapped to: {scispacy_semantic_type}")
                
                # If we have both semantic type and definition from SciSpacy, use them
                if scispacy_semantic_type and scispacy_definition and scispacy_definition.strip():
                    logger.debug(f"Using complete SciSpacy info for CUI {cui}: {scispacy_semantic_type}")
                    return scispacy_semantic_type, scispacy_definition
                
                # If we have SciSpacy semantic type but no definition, try joined.txt for definition
                if scispacy_semantic_type:
                    if cui in self.cui_to_semantic_name:
                        semantic_name = self.cui_to_semantic_name[cui]
                        joined_definition = self.semantic_type_definitions.get(semantic_name, "")
                        
                        if joined_definition and joined_definition.strip():
                            logger.debug(f"Using SciSpacy type + joined.txt definition for CUI {cui}")
                            return scispacy_semantic_type, joined_definition
                    
                    # No definition available from either source
                    logger.debug(f"No definition available for CUI {cui}, using empty string")
                    return scispacy_semantic_type, "No definition available for this entity."
        
        # Second try: Fallback to joined.txt if SciSpacy completely fails
        if cui in self.cui_to_semantic_name:
            semantic_name = self.cui_to_semantic_name[cui]
            joined_semantic_type = self.semantic_type_categories.get(semantic_name, "Entity")
            joined_definition = self.semantic_type_definitions.get(semantic_name, "")
            
            logger.debug(f"Using joined.txt fallback for CUI {cui}: {joined_semantic_type}")
            return joined_semantic_type, joined_definition
        
        # Final fallback: CUI not found in either source
        logger.debug(f"CUI {cui} not found in SciSpacy or joined.txt")
        return "Entity", ""
    
    def _map_tui_to_semantic_type(self, tui: str, canonical_name: str = "") -> str:
        """Comprehensive TUI to semantic type mapping based on UMLS semantic network."""
        
        # Comprehensive TUI mappings - based on UMLS Semantic Network documentation
        tui_mappings = {
            # ENTITIES - Organisms
            'T001': 'Organism',
            'T002': 'Plant',
            'T004': 'Fungus',
            'T005': 'Virus',
            'T007': 'Bacterium',
            'T008': 'Animal',
            'T010': 'Vertebrate',
            'T011': 'Amphibian',
            'T012': 'Bird',
            'T013': 'Fish',
            'T014': 'Reptile',
            'T015': 'Mammal',
            'T016': 'Human',
            
            # ENTITIES - Anatomical Structures
            'T017': 'Anatomical Structure',
            'T018': 'Embryonic Structure',
            'T019': 'Congenital Abnormality',
            'T020': 'Acquired Abnormality',
            'T021': 'Fully Formed Anatomical Structure',
            'T022': 'Body System',
            'T023': 'Body Part, Organ, or Organ Component',
            'T024': 'Tissue',
            'T025': 'Cell',
            'T026': 'Cell Component',
            'T029': 'Body Location or Region',
            'T030': 'Body Space or Junction',
            
            # ENTITIES - Biological Function
            'T031': 'Body Substance',
            'T032': 'Organism Attribute',
            'T033': 'Finding',
            'T034': 'Laboratory or Test Result',
            'T037': 'Injury or Poisoning',
            'T038': 'Biologic Function',
            'T039': 'Physiologic Function',
            'T040': 'Organism Function',
            'T041': 'Mental Process',
            'T042': 'Organ or Tissue Function',
            'T043': 'Cell Function',
            'T044': 'Molecular Function',
            'T045': 'Genetic Function',
            
            # ENTITIES - Chemicals & Drugs
            'T103': 'Chemical',
            'T104': 'Chemical Viewed Structurally',
            'T109': 'Organic Chemical',
            'T110': 'Steroid',
            'T111': 'Eicosanoid',
            'T114': 'Nucleic Acid, Nucleoside, or Nucleotide',
            'T115': 'Organophosphorus Compound',
            'T116': 'Amino Acid, Peptide, or Protein',
            'T117': 'Carbohydrate',
            'T118': 'Carboxylic Acid',
            'T119': 'Lipid',
            'T120': 'Chemical Viewed Functionally',
            'T121': 'Pharmacologic Substance',
            'T122': 'Biomedical or Dental Material',
            'T123': 'Biologically Active Substance',
            'T124': 'Neuroreactive Substance or Biogenic Amine',
            'T125': 'Hormone',
            'T126': 'Enzyme',
            'T127': 'Vitamin',
            'T129': 'Immunologic Factor',
            'T130': 'Indicator, Reagent, or Diagnostic Aid',
            'T131': 'Hazardous or Poisonous Substance',
            'T195': 'Antibiotic',
            'T196': 'Element, Ion, or Isotope',
            'T197': 'Inorganic Chemical',
            'T200': 'Clinical Drug',
            'T203': 'Drug Delivery Device',
            
            # ENTITIES - Events
            'T046': 'Pathologic Function',
            'T047': 'Disease or Syndrome',
            'T048': 'Mental or Behavioral Dysfunction',
            'T049': 'Cell or Molecular Dysfunction',
            'T050': 'Experimental Model of Disease',
            'T051': 'Event',
            'T052': 'Activity',
            'T053': 'Behavior',
            'T054': 'Social Behavior',
            'T055': 'Individual Behavior',
            'T056': 'Daily or Recreational Activity',
            'T057': 'Occupational Activity',
            'T058': 'Health Care Activity',
            'T059': 'Laboratory Procedure',
            'T060': 'Diagnostic Procedure',
            'T061': 'Therapeutic or Preventive Procedure',
            'T062': 'Research Activity',
            'T063': 'Molecular Biology Research Technique',
            'T064': 'Governmental or Regulatory Activity',
            'T065': 'Educational Activity',
            'T066': 'Machine Activity',
            'T067': 'Phenomenon or Process',
            
            # ENTITIES - Physical Objects
            'T071': 'Entity',
            'T072': 'Physical Object',
            'T073': 'Manufactured Object',
            'T074': 'Medical Device',
            'T075': 'Research Device',
            'T080': 'Qualitative Concept',
            'T081': 'Quantitative Concept',
            'T082': 'Temporal Concept',
            'T083': 'Geographic Area',
            'T085': 'Molecular Sequence',
            
            # ENTITIES - Concepts & Ideas  
            'T078': 'Idea or Concept',
            'T079': 'Temporal Concept',
            'T086': 'Nucleotide Sequence',
            'T087': 'Amino Acid Sequence',
            'T088': 'Carbohydrate Sequence',
            'T089': 'Regulation or Law',
            'T090': 'Occupation or Discipline',
            'T091': 'Biomedical Occupation or Discipline',
            'T092': 'Organization',
            'T093': 'Health Care Related Organization',
            'T094': 'Professional Society',
            'T095': 'Self-help or Relief Organization',
            'T096': 'Group',
            'T097': 'Professional or Occupational Group',
            'T098': 'Population Group',
            'T099': 'Family Group',
            'T100': 'Age Group',
            'T101': 'Patient or Disabled Group',
            'T102': 'Group Attribute',
            
            # ENTITIES - Living Beings
            'T204': 'Eukaryote',
            'T168': 'Food',
            'T167': 'Substance',
            'T170': 'Intellectual Product',
            'T171': 'Language',
            'T169': 'Functional Concept',
            'T184': 'Sign or Symptom',
            'T185': 'Classification',
            'T190': 'Anatomical Abnormality',
            'T191': 'Neoplastic Process',
            'T201': 'Clinical Attribute',
            'T202': 'Drug or Biological Response',
        }
        
        # Return the mapped semantic type, or a descriptive fallback
        if tui in tui_mappings:
            return tui_mappings[tui]
        
        # Enhanced fallback using canonical name analysis
        if canonical_name:
            name_lower = canonical_name.lower()
            
            # Medical procedure indicators
            if any(word in name_lower for word in ['procedure', 'surgery', 'operation', 'treatment', 'therapy']):
                return 'Therapeutic or Preventive Procedure'
            # Patient/group indicators
            elif any(word in name_lower for word in ['patient', 'group', 'population']):
                return 'Patient or Disabled Group'
            # Disease/condition indicators  
            elif any(word in name_lower for word in ['disease', 'disorder', 'syndrome', 'condition']):
                return 'Disease or Syndrome'
            # Chemical/drug indicators
            elif any(word in name_lower for word in ['drug', 'chemical', 'substance', 'compound']):
                return 'Chemical'
            # Finding/result indicators
            elif any(word in name_lower for word in ['finding', 'result', 'test', 'measurement']):
                return 'Finding'
            # Anatomical indicators
            elif any(word in name_lower for word in ['organ', 'tissue', 'cell', 'body', 'anatomical']):
                return 'Anatomical Structure'
        
        # Final fallback - return unknown TUI for debugging
        return f"Unknown Semantic Type ({tui})"
    
    def extract_entities_with_improved_cui(self, paragraph_content: str) -> Dict[str, Any]:
        """
        Extract entities with improved CUI assignment using separated medical entity extractor.
        
        Medical Text Interpretation:
        - Clinical discharge notes: Focus on patient-specific diagnoses, medications, procedures
        - Medical textbooks: Emphasize anatomical structures, disease classifications, treatments
        - Clinical labs: Highlight test results, measurements, reference values, abnormalities
        - Clinical guidelines: Target treatment recommendations, contraindications, protocols
        
        Maintains original JSON output structure for compatibility.
        """
        return self.entity_extractor.extract_medical_entities(paragraph_content)
    
    
    
    def _parse_llm_response(self, raw) -> dict:
        """Parse LLM response to extract JSON."""
        try:
            if isinstance(raw, dict):
                return raw
            
            txt = str(raw).strip()
            txt = re.sub(r"^```(?:json)?|```$", "", txt, flags=re.IGNORECASE).strip()
            
            try:
                data = json.loads(txt)
            except json.JSONDecodeError:
                try:
                    data = json.loads(txt.replace("'", '"'))
                except json.JSONDecodeError:
                    try:
                        data = ast.literal_eval(txt)
                    except:
                        data = {}
            
            if isinstance(data, list):
                data = {"entities": data}
            
            return {"entities": data.get("entities", [])}
            
        except Exception as e:
            logger.error(f"Failed to parse LLM response: {e}")
            return {"entities": []}
    
    def _parse_cui_selection_response(self, raw) -> dict:
        """Parse LLM response for CUI selection (different format than entity descriptions)."""
        try:
            if isinstance(raw, dict):
                return raw
            
            txt = str(raw).strip()
            txt = re.sub(r"^```(?:json)?|```$", "", txt, flags=re.IGNORECASE).strip()
            
            # Try multiple parsing approaches
            try:
                data = json.loads(txt)
            except json.JSONDecodeError:
                try:
                    data = json.loads(txt.replace("'", '"'))
                except json.JSONDecodeError:
                    try:
                        data = ast.literal_eval(txt)
                    except:
                        logger.warning(f"Failed to parse CUI selection response: {txt[:200]}...")
                        return {}
            
            # CUI selection response should have selected_cui and reasoning
            if isinstance(data, dict) and "selected_cui" in data:
                return data
            else:
                logger.warning(f"CUI selection response missing 'selected_cui': {data}")
                return {}
            
        except Exception as e:
            logger.error(f"Failed to parse CUI selection response: {e}")
            return {}
    
    
    
    
    
    
    def load_semantic_types(self, txt_path: str) -> None:
        """Load semantic types from pipe-delimited text file."""
        try:
            semantic_types = []
            semantic_type_definitions = {}
            semantic_type_categories = {}
            semantic_type_cuis = {}
            semantic_type_tuis = {}
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and '|' in line:
                        parts = line.split('|', 4)
                        if len(parts) >= 4:
                            cui = parts[0].strip()
                            entity_name = parts[1].strip()
                            entity_type = parts[2].strip()
                            definition = parts[3].strip() if len(parts) > 3 else ""
                            tui = parts[4].strip() if len(parts) > 4 else ""
                            
                            semantic_types.append(entity_name)
                            semantic_type_definitions[entity_name] = definition
                            semantic_type_categories[entity_name] = entity_type
                            semantic_type_cuis[entity_name] = cui
                            semantic_type_tuis[entity_name] = tui
            
            self.semantic_types = semantic_types
            self.semantic_type_definitions = semantic_type_definitions
            self.semantic_type_categories = semantic_type_categories
            self.semantic_type_cuis = semantic_type_cuis
            self.semantic_type_tuis = semantic_type_tuis
            self.cui_to_semantic_name = {cui: name for name, cui in semantic_type_cuis.items()}
            
            logger.info(f" Loaded {len(self.semantic_types)} semantic types")
            
        except Exception as e:
            logger.error(f"Failed to load semantic types: {e}")
            raise
    
    def _load_description_cache(self) -> None:
        """Load entity description cache from disk."""
        try:
            cache_dir = os.path.dirname(self.cache_path)
            os.makedirs(cache_dir, exist_ok=True)
            if os.path.exists(self.cache_path):
                with open(self.cache_path, "r", encoding="utf-8") as f:
                    self.desc_cache = json.load(f)
                    logger.info(f"[OK] Loaded description cache with {len(self.desc_cache)} entries")
        except Exception as e:
            logger.warning(f"Failed to load description cache: {e}")
            self.desc_cache = {}
    
    def _save_description_cache(self) -> None:
        """Save description cache to disk."""
        try:
            with self.cache_lock:
                cache_dir = os.path.dirname(self.cache_path)
                os.makedirs(cache_dir, exist_ok=True)
                with open(self.cache_path, "w", encoding="utf-8") as f:
                    json.dump(self.desc_cache, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"Failed to save description cache: {e}")
    
    def process_chunked_json(self, json_path: str, max_paragraphs: int = 20, document_type: str = None) -> Dict[str, Any]:
        """Process chunked JSON file with improved CUI assignment."""
        try:
            # Load JSON file
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f" Loaded JSON file with {len(data.get('chunks', []))} chunks")
            
            # Collect paragraphs
            all_paragraphs = []
            for chunk in data.get('chunks', []):
                chunk_number = chunk.get('chunk_number')
                paragraphs = chunk.get('paragraphs', [])
                for idx, paragraph in enumerate(paragraphs, start=1):
                    if paragraph.get('type') == 'text' and paragraph.get('content', '').strip():
                        all_paragraphs.append({
                            'chunk_number': chunk_number,
                            'content': paragraph.get('content'),
                            'type': paragraph.get('type'),
                            'paragraph_index': idx
                        })
            
            # Limit to first N paragraphs
            paragraphs_to_process = all_paragraphs[:max_paragraphs]
            logger.info(f"Processing {len(paragraphs_to_process)} paragraphs with improved CUI assignment")
            
            results = {
                'metadata': {
                    'total_paragraphs_processed': len(paragraphs_to_process),
                    'spacy_model': self.spacy_model,
                    'ollama_model': self.ollama_model,
                    'semantic_types_loaded': len(self.semantic_types),
                    'document_name': os.path.basename(json_path),
                    'document_type': document_type,
                    'cui_assignment_method': 'llm-enhanced' if self.use_description_context else 'standard'
                },
                'chunks': {}
            }
            
            # Process each paragraph with improved CUI assignment
            for idx, paragraph in enumerate(paragraphs_to_process):
                logger.info(f"Processing paragraph {idx+1}/{len(paragraphs_to_process)}")
                
                extraction_results = self.extract_entities_with_improved_cui(paragraph['content'])
                entities = extraction_results.get('entities', [])
                
                # Organize by chunk
                chunk_number = paragraph['chunk_number']
                if chunk_number not in results['chunks']:
                    results['chunks'][chunk_number] = {
                        'paragraphs': [],
                        'relationships_within_chunk': [],
                        'medical_summary_for_chunk': {},
                        'medical_summary_embeddings_for_chunk': {}
                    }
                
                results['chunks'][chunk_number]['paragraphs'].append({
                    'paragraph_index': paragraph['paragraph_index'],
                    'content': paragraph['content'],
                    'entities': entities,
                    'entity_count': len(entities)
                })
                
                if entities:
                    logger.info(f"  Found {len(entities)} entities with improved CUI assignment")
                    # Log some examples of CUI assignments
                    for entity in entities[:3]:  # Show first 3
                        logger.debug(f"    - {entity['entity_name']}: CUI={entity['cui']}, Type={entity['entity_type']}")
            
            # Generate relationships and medical summaries for each chunk
            logger.info("Generating relationships and medical summaries for each chunk...")
            for chunk_number, chunk_data in results['chunks'].items():
                # Aggregate all entities in the chunk
                all_chunk_entities = []
                chunk_text = ""
                for paragraph in chunk_data['paragraphs']:
                    all_chunk_entities.extend(paragraph['entities'])
                    chunk_text += paragraph['content'] + " "
                
                # Generate relationships using the new relationship generator
                if self.relationship_generator and all_chunk_entities:
                    logger.info(f"Generating relationships for chunk {chunk_number} with {len(all_chunk_entities)} entities")
                    chunk_relationships = self.relationship_generator.generate_relationships(
                        entities=all_chunk_entities,
                        chunk_text=chunk_text
                    )
                    chunk_data['relationships_within_chunk'] = chunk_relationships
                    logger.info(f"Generated {len(chunk_relationships)} relationships for chunk {chunk_number}")
                else:
                    logger.info(f"No relationship generator available or no entities for chunk {chunk_number}")
                    chunk_data['relationships_within_chunk'] = []
                
                # Generate medical summary if summarizer is available
                if self.summarizer is not None and all_chunk_entities:
                    logger.info(f"Generating medical summary for chunk {chunk_number} with {len(all_chunk_entities)} entities")
                    try:
                        chunk_summary = self.summarizer.summarize_chunk(
                            chunk_text, all_chunk_entities, []
                        ) or {}
                        if chunk_summary:
                            logger.info(f"Generated medical summary with {len(chunk_summary)} categories for chunk {chunk_number}")
                        else:
                            logger.warning(f"Empty medical summary returned for chunk {chunk_number}")
                    except Exception as e:
                        logger.error(f"Error generating medical summary for chunk {chunk_number}: {e}", exc_info=True)
                        chunk_summary = {}
                    chunk_data['medical_summary_for_chunk'] = chunk_summary
                    
                    # Generate embeddings for the medical summary categories
                    if self.embedder is not None and chunk_summary:
                        try:
                            medical_summary_embeddings = {}
                            for category, summary_text in chunk_summary.items():
                                if summary_text and str(summary_text).strip():
                                    category_embedding = self.embedder.encode([str(summary_text)])
                                    if len(category_embedding) > 0:
                                        medical_summary_embeddings[category] = category_embedding[0].tolist()
                            
                            chunk_data['medical_summary_embeddings_for_chunk'] = medical_summary_embeddings
                            logger.debug(f"Generated embeddings for {len(medical_summary_embeddings)} medical categories in chunk {chunk_number}")
                        except Exception as e:
                            logger.warning(f"Failed to generate medical summary embeddings for chunk {chunk_number}: {e}")
                            chunk_data['medical_summary_embeddings_for_chunk'] = {}
                    else:
                        chunk_data['medical_summary_embeddings_for_chunk'] = {}
                else:
                    if self.summarizer is None:
                        logger.warning(f"Summarizer is None, skipping medical summary for chunk {chunk_number}")
                    elif not all_chunk_entities:
                        logger.info(f"No entities found in chunk {chunk_number}, skipping medical summary")
                    chunk_data['medical_summary_for_chunk'] = {}
                    chunk_data['medical_summary_embeddings_for_chunk'] = {}
            
            # Add completion metadata
            total_entities_found = 0
            total_paragraphs_with_entities = 0
            
            for chunk in results.get('chunks', {}).values():
                for p in chunk.get('paragraphs', []):
                    ent_count = len(p.get('entities', []))
                    total_entities_found += ent_count
                    if ent_count > 0:
                        total_paragraphs_with_entities += 1
            
            results['completion_metadata'] = {
                'completed_at': datetime.datetime.now().strftime("%Y%m%d_%H%M%S"),
                'status': 'completed',
                'total_entities_found': total_entities_found,
                'total_chunks': len(results.get('chunks', {})),
                'total_paragraphs_processed': results.get('metadata', {}).get('total_paragraphs_processed', 0),
                'paragraphs_with_entities': total_paragraphs_with_entities,
            }
            
            logger.info(f" Processing completed: {total_entities_found} entities with LLM-enhanced CUI assignment")
            return results
            
        except Exception as e:
            logger.error(f"Failed to process chunked JSON: {e}")
            raise


def main():
    """Main function to run the improved entity processor."""
    import sys
    import glob
    
    # Defaults
    default_folder = "docs-for-processing"
    default_txt = "joined.txt"
    default_max_paragraphs = 10
    default_spacy = "en_core_sci_md"
    default_llm = "llama3.1:8b"
    
    try:
        print("\n=== Improved Entity Processor with Context-Enhanced CUI Assignment ===\n")
        print("This version processes all chunked JSON files in a folder and classifies them by document type.\n")
        print("Press Enter to accept defaults shown in [brackets].\n")
        
        folder_path = input(f"Folder path containing chunked JSON files [{default_folder}]: ").strip() or default_folder
        
        # Find all chunked JSON files in the folder
        if not os.path.exists(folder_path):
            print(f"Error: Folder '{folder_path}' does not exist.")
            sys.exit(1)
            
        json_files = glob.glob(os.path.join(folder_path, "*_chunked.json"))
        if not json_files:
            print(f"No chunked JSON files found in '{folder_path}'. Looking for files ending with '_chunked.json'")
            sys.exit(1)
            
        print(f"Found {len(json_files)} chunked JSON files:")
        for i, file in enumerate(json_files, 1):
            print(f"  {i}. {os.path.basename(file)}")
        print()
        
        # Classify document types for all files during initialization
        print("Document Type Classification:")
        print("  - rag-document: A document for RAG (Retrieval Augmented Generation) applications")
        print("  - corpus-document: A document that's part of a research/training corpus")
        print()
        
        document_types = {}
        for json_file in json_files:
            filename = os.path.basename(json_file)
            
            while True:
                doc_type = input(f"Is '{filename}' a rag-document or corpus-document? (rag/corpus): ").strip().lower()
                if doc_type in ['rag', 'corpus']:
                    document_types[json_file] = f"{doc_type}-document"
                    break
                else:
                    print("Please enter 'rag' or 'corpus'")
        
        print(f"\n Document types configured for {len(document_types)} files")
        print()
        
        txt_path = input(f"Semantic types path [{default_txt}]: ").strip() or default_txt
        max_para_str = input(f"Max paragraphs [{default_max_paragraphs}]: ").strip()
        max_paragraphs = int(max_para_str) if max_para_str else default_max_paragraphs
        
        # Relationship batch size configuration not needed for current implementation
        # default_batch_size = 4
        # batch_size_str = input(f"Relationship batch size [{default_batch_size}]: ").strip()
        # relationship_batch_size = int(batch_size_str) if batch_size_str else default_batch_size
        
        spacy_model = input(f"SpaCy model [{default_spacy}]: ").strip() or default_spacy
        llm_model = input(f"LLM model [{default_llm}]: ").strip() or default_llm
        
        # Ask about context-enhanced CUI assignment
        use_context = input("Use description context for CUI assignment? (y/n) [y]: ").strip().lower()
        use_description_context = use_context != 'n'
        
        # Ask for endpoint
        endpoint = "http://localhost:11434"
        if input("Use LM Studio? (y/n) [n]: ").lower().startswith('y'):
            endpoint = input("LM Studio endpoint [http://127.0.0.1:1234]: ").strip() or "http://127.0.0.1:1234"
            if not llm_model or llm_model.lower() in ["lmstudio", "lm studio"]:
                llm_model = input("LM Studio model identifier [mmed-llama-3-8b]: ").strip() or "mmed-llama-3-8b"
            print(f"Using LM Studio at {endpoint} with model: {llm_model}")
        else:
            print(f"Using Ollama model: {llm_model}")
        
        # Initialize processor with relationships enabled
        processor = ImprovedEntityProcessor(
            spacy_model=spacy_model,
            ollama_model=llm_model,
            ollama_endpoint=endpoint,
            use_description_context=use_description_context,
        )
        
        # Load semantic types
        processor.load_semantic_types(txt_path)
        
        # Process each JSON file using pre-determined document types
        print("Now processing each file using the configured document types:")
        print()
        
        for json_file in json_files:
            filename = os.path.basename(json_file)
            document_type = document_types[json_file]
            
            print(f"Processing: {filename} as {document_type}...")
            
            # Process the file
            results = processor.process_chunked_json(json_file, max_paragraphs, document_type=document_type)
            
            # Generate output filename from first 5 characters of input document
            base_name = os.path.splitext(filename)[0]  # Remove .json extension
            if base_name.endswith('_chunked'):
                base_name = base_name[:-8]  # Remove _chunked suffix
            output_prefix = base_name[:5].replace(' ', '_').replace('-', '_')
            output_file = f"{output_prefix}_entity_processing_results.json"
            
            # Save results
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f" Results saved to {output_file}")
            
            # Print summary for this file
            total_paragraphs = results['metadata']['total_paragraphs_processed']
            total_chunks = results.get('completion_metadata', {}).get('total_chunks', 0)
            paragraphs_with_entities = results.get('completion_metadata', {}).get('paragraphs_with_entities', 0)
            print(f"  Processed {total_paragraphs} paragraphs across {total_chunks} chunks")
            print(f"  Found entities in {paragraphs_with_entities} paragraphs")
            print(f"  Document type: {document_type}")
            print(f"  CUI assignment method: {results['metadata']['cui_assignment_method']}")
            print()
        
        print(f" Batch processing complete! Processed {len(json_files)} files.")
        
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()